---
editor_options: 
  markdown: 
    wrap: 72
---

\newpage

# LAB 4: Testing for multivariate normality

## Generation of random multivariate Gaussian sample

```{r}
library(mvtnorm)
library(mvnormtest)
library(car) # to draw ellipses
```

We generate a sample of `n=150` observation from bivariate Gaussian (as in `LAB_3.R`)

```{r}
mu  <- c(1,2)
sig <- matrix(c(1,1,1,2), 2)
n   <- 150

set.seed(12345)
X <- rmvnorm(n, mu, sig)

x.1 <- seq(-4,6,0.15)
x.2 <- seq(-4,8,0.15)
w <- matrix(NA, length(x.1), length(x.2)) # create matrix of NAs
for(i in 1:length(x.1)){  
  for(j in 1:length(x.2)){
    w[i,j] <- dmvnorm(c(x.1[i],x.2[j]),mu,sig)
  }
}
image(x.1, x.2, w,asp=1, ylim=c(-4,8), main="Sample points")
points(X[,1],X[,2],pch=20,cex=.75)
```

## Test of normality

### Approach 1: look at some linear combinations of the original variables

Example: original variables, principal components

We first do some visualization

```{r}

par(mfrow=c(2,2))

hist(X[,1], prob=T, ylab='density', xlab='X.1', main='Histogram of X.1',ylim=c(0,0.45))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(X[,1]),sd(X[,1])), col='blue', lty=2)

hist(X[,2], prob=T, ylab='density', xlab='X.2', main='Histogram of X.2',ylim=c(0,0.45))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(X[,2]),sd(X[,2])), col='blue', lty=2)

qqnorm(X[,1], main='QQplot of X.1',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(X[,1])

qqnorm(X[,2], main='QQplot of X.2',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(X[,2])
```

We perform univariate tests of normality on the two components

```{r}
shapiro.test(X[,1])
shapiro.test(X[,2])
```

Recall: **Shapiro-Wilk test**

$H_0: X \sim N$ vs $H_1: H_0^c$

Test statistics:
$$W=\frac{(\text{angular coeff. of the qqline})^2}{\text{sample variance}}$$
One can prove that:

-   $w<1$
-   the empirical distribution under $H_0$ is concentrated on values
    near 1
    -   (if $n=50$ more than 90% of the obs. is between 0.95 and 1);
    -   (if $n=5$ more than 90% of the obs. is between 0.81 and 1).

If the data do NOT come from a Gaussian distribution, the distribution of the test statistics moves towards smaller values: small values of the statistics W give evidence against $H_0$

**Reject** $H_0$ for small values of $W$.

We look at the directions of the PCs

```{r}
pc.X <- princomp(X,scores=T)
head(pc.X$scores)

par(mfrow=c(2,2))

hist(pc.X$scores[,1], prob=T, ylab='density', xlab='comp.1', main='Histogram of PC1',ylim=c(0,0.41))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(pc.X$scores[,1]),sd(pc.X$scores[,1])), col='blue', lty=2)

hist(pc.X$scores[,2], prob=T, ylab='density', xlab='comp.2', main='Histogram of PC2',ylim=c(0,0.7))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(pc.X$scores[,2]),sd(pc.X$scores[,2])), col='blue', lty=2)

qqnorm(pc.X$scores[,1], main='QQplot of PC1',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(pc.X$scores[,1])

qqnorm(pc.X$scores[,2], main='QQplot of PC2',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(pc.X$scores[,2])

shapiro.test(pc.X$scores[,1])
shapiro.test(pc.X$scores[,2])
```

**Problem: which level should I use in each test, to get a global level of** $\alpha$?

### Approach 2: Consider the squared Mahalanobis distances of the data from the (sample) mean and test if they are a sample from a chi-square distribution

Recall this Theorem:
$$X\sim N (\mu,\Sigma)\in R^p,\det(\Sigma)>0 \implies d^2(X,\mu)=(X-\mu)'\Sigma^{-1}(X-\mu)\sim\chi^2(p)$$

```{r}

plot(X, asp=1,xlab='X.1',ylab='X.2')
for(prob in (1:9)/10)
  dataEllipse(X, levels = prob , add=T)

d2 <- mahalanobis(X, colMeans(X), cov(X))

par(mfrow=c(1,2))
hist(d2, prob=T, main='Histogram of the Mahalanobis dist.',
     xlab='d2',ylab='density', col='grey84')
lines(0:2000/100, dchisq(0:2000/100,2), col='blue', lty=2, lwd=2)
qqplot(qchisq((1:n - 0.5)/n, df = 2), d2, main='QQplot of (sample) d2',xlab='theoretical quantiles Chi-sq(2)',
       ylab='sample quantiles')
abline(0, 1)
```
### we can perform a chi.sq goodness-of-fit test

d2.class <- cut(d2, qchisq((0:10)/10, df = 2))
d2.freq  <- table(d2.class)

chisq.test(x = d2.freq, p = rep(1/10, 10), simulate.p.value = T)
# Test: does the population probabilities (given in x) equal those in p?

# Remark: since the mean and covariance matrix are unknown, we can only
#     have approximate solutions:
#     the Mahalanobis distance is computed with estimates of the mean vector
#     and of the covariance matrix; the sample of distances is not iid

graphics.off()

#_______________________________________________________________________________
### Approach 3: test of all the directions simultaneously, by looking at the min
### of the Shapiro-Wilk statistics

# We reject H0: X ~ N if we observe a "low" value of W along at least 
# one direction, i.e., if the minimum of W along the direction is "low" 

# Looking at all directions, is equivalent to looking at min(W) along
# the directions (test statistic)

# Example with our simulated data: we compute the W statistics for all
# the directions.
theta   <- seq(0, pi - pi/180, by = pi/180)
W       <- NULL
P       <- NULL
for(i in 1:length(theta))
{
  a   <- c(cos(theta[i]), sin(theta[i]))
  w   <- shapiro.test(X %*% a)$statistic
  p   <- shapiro.test(X %*% a)$p.value
  W   <- c(W, w)
  P   <- c(P, p)
}

x11()
par(mfrow = c(2,1))
plot(theta, W, main = 'W statistics', ylim = c(0.95,1), type='l')
abline(v=c(0, pi/2), col = 'blue')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]), col='red')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]) + pi, col='red')

plot(theta, P, main = 'P-values', ylim = c(0,1), type='l')
abline(v=c(0, pi/2), col = 'blue')
abline(h=0.10, col = 'blue', lty = 2) # set alpha=10%
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]), col='red')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]) + pi, col='red')

# Note: to set the rejection region, we should look at the distribution of 
# of min(W) under H0 [not the distribution of W.a for all the directions a]
# --> To see how much we reject globally if we set a threshold
#     alpha=10% at the univariate tests based on W.a, see Experiment.R

# Hence, to build the rejection region for the test, 
# we just need to set the threshold as the quantile of order alpha
# of the distribution of min(W) under H0

# Formally: 
# H0: X ~ N vs H1: H0^c
# test statistics: min(W) ~ F under H0
# Reject H0 if min(W)<qF(alpha), 
# with qF(alpha) s.t. P(min(W) < qF(alpha)|H0) = alpha

# The distribution F of min(W) is not known, but it can be approximated with
# a Monte Carlo method. That is, we approximate the distribution of min(W) with a 
# histogram generated by simulating Gaussian samples. The quantile qF(alpha) is
# estimated with the sample quantile of order alpha from the samples.

# Note: an explicit expression is available for min(W). It is computed 
# by the function mshapiro.test 

# Example: for 200 sample we can compute min(W) and look at its distribution
min.W=NULL
for(i in 1:200)
{
  Y <- rmvnorm(n, mu, sig)
  min.W <- c(min.W, mshapiro.test(t(Y))$stat)
}
x11()
hist(min.W, prob=T, col='grey81', main='Histogram of min(W)'); box()
abline(v=quantile(min.W, probs = .1), col=2)
text(quantile(min.W, probs = .1), 85, labels = expression(q[F](1-alpha)),
     pos=2)

# => I'll reject H0 if min(W)<qF(alpha), with qF(alpha)~.98
quantile(min.W, probs = .1)

# Actual observation of min(W)
min.W0 = mshapiro.test(t(X))$stat
min.W0 # accept H0

abline(v=min.W0, col='blue')

# Approximate the p-value with Monte Carlo: count how many realizations under H0
# are associated with a min(W) lower than the actual observation

# Proportion of the realization that has min(W) lower than min.W0
sum(min.W < min.W0)/200

# Very high p-value => accept H0

### The function mcshapiro.test implements a procedure that:
### 1- approximates the distribution of the statistics min(W) via MC
### 2- performs a test of normality based on the (approximate) distribution of min(W)
### 3- returns an approximate p-value of the test at point 2-
mcshapiro.test <- function(X, devstmax = 0.01, sim = ceiling(1/(4*devstmax^2)))
{
  library(mvnormtest)
  n   <- dim(X)[1]
  p   <- dim(X)[2]
  mu  <- rep(0,p)
  sig <- diag(p)
  W   <- NULL
  for(i in 1:sim)
  {
    Xsim <- rmvnorm(n, mu, sig)
    W   <- c(W, mshapiro.test(t(Xsim))$stat)
    # mshapiro.test(X): compute the statistics min(W) for the sample X
  }
  Wmin   <- mshapiro.test(t(X))$stat   # min(W) for the given sample
  pvalue <- sum(W < Wmin)/sim          # proportion of min(W) more extreme than the observed Wmin
  devst  <- sqrt(pvalue*(1-pvalue)/sim)
  list(Wmin = as.vector(Wmin), pvalue = pvalue, devst = devst, sim = sim)
}

mcshapiro.test(X)

graphics.off()

#_______________________________________________________________________________
# Example: test of normality for the dataset stiff
# Dataset: each sample unit is a board. For each board, four measures of 
# stiffness are taken (X1: sending a shock wave, X2: while vibrating the board, 
# X3 and X4: static tests)

stiff <- read.table('stiff.dat')
stiff

x11()
plot(stiff, asp=1, pch=19)

# Normality of the components
x11(width=12)
par(mfcol=c(2,4))

for(i in 1:4)
{
  hist(stiff[,i], prob=T, main=paste('Histogram of V', i, sep=''), xlab=paste('V', i, sep=''))
  lines(900:2800, dnorm(900:2800,mean(stiff[,i]),sd(stiff[,i])), col='blue', lty=2)
  qqnorm(stiff[,i], main=paste('QQplot of V', i, sep=''))
  qqline(stiff[,i])
  print(shapiro.test(stiff[,i])$p)
}

# Normality of the principal components
PCs <- data.frame(princomp(stiff)$scores)

x11()
plot(PCs, asp=1, pch=19)

x11(width=13)
par(mfcol=c(2,4))
for(i in 1:4)
{
  hist(PCs[,i], prob=T, main=paste('Histogram of PC', i, sep=''))
  lines(seq(min(PCs[,i]), max(PCs[,i]), length=2000), dnorm(seq(min(PCs[,i]), max(PCs[,i]), length=2000),mean(PCs[,i]),sd(PCs[,i])), col='blue', lty=2)
  qqnorm(PCs[,i], main=paste('QQplot of PC', i, sep=''))
  qqline(PCs[,i])
  print(shapiro.test(PCs[,i])$p)
}

# Mahalanobis distances of the data from the sample mean
M <- colMeans(stiff)
S <- cov(stiff)

d2 <- matrix(mahalanobis(stiff, M, S))

windows()
par(mfrow=c(1,2))

hist(d2, prob=T)
lines(0:2000/100, dchisq(0:2000/100,4), col='blue', lty=2)

qqplot(qchisq(seq(0.5/30, 1 - 0.5/30 ,by = 1/30), df = 4), d2,  main='QQplot di d2')
abline(0, 1)

d2.class <- cut(d2, qchisq((0:6)/6, df = 4))
d2.freq  <- table(d2.class)

chisq.test(x = d2.freq, p = rep(1/6, 6), simulate.p.value = T)

# test of all the directions simultaneously
mcshapiro.test(stiff)

### The data don't seem Gaussian. What can we do?
### Identify clusters 
### Identify (and possibly remove) outliers
### Transform the data (e.g., Box-Cox transformations, see Johnson-Wichern Chap.4.8,
###                     R functions powerTransform(); bcPower())
### Work without the Gaussian assumption (e.g., permutation tests)

### Let's try to identify and remove outliers:
# We remove the data too far (in the sense of the Mahalanobis distance) 
# from the center of the distribution

x11()
plot(d2, pch=ifelse(d2<7.5,1,19))
x11()
plot(stiff, pch=ifelse(d2<7.5,1,19))

stiff.noout <- stiff[which(d2<7.5),]

mcshapiro.test(stiff.noout)

# In this case removing the outliers solves the problem of non-gaussianity

